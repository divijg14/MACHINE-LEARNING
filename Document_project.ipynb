{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Manual implementation starts here </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> List of Stop Words </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fd = open(\"D:/document_project/stopwords.txt\",\"r\")\n",
    "strr = \"\"\n",
    "stop_words = []\n",
    "strr = (fd.readlines())\n",
    "for i in strr:\n",
    "    stop_words.append(i[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> training of data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "path = \"D:/document_project/mini_newsgroups\"\n",
    "word_list= {}\n",
    "file_names = {}\n",
    "voc = {}\n",
    "class_counts = {}\n",
    "for filename in os.listdir(path):\n",
    "    new_path = os.path.join(path, filename)\n",
    "    word_list[filename]={}\n",
    "    \n",
    "    file_names[filename] = []\n",
    "    \n",
    "    class_counts[filename] = 0\n",
    "    for textfiles in os.listdir(new_path):\n",
    "        class_counts[filename] +=1\n",
    "        file_names[filename].append(textfiles)\n",
    "        file = os.path.join(new_path, textfiles)\n",
    "        f = open(file,\"r\")\n",
    "        for line in f:\n",
    "            if 'Lines' in line:\n",
    "                break\n",
    "        for line in f:\n",
    "            for word in line.replace(',','').replace('>','').replace(']','').replace('[','').replace(':','').replace('\\'','').replace('.','').replace('(','').replace(')','').lower().split():\n",
    "                if word not in stop_words:\n",
    "                    if word not in word_list[filename]:\n",
    "                        word_list[filename][word]=1\n",
    "                        voc[word]=1\n",
    "                    elif word in word_list[filename]:\n",
    "                        word_list[filename][word] +=1\n",
    "                        voc[word] += 1\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predict single class probability</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSingleClassProb(file,word_list1 , word_list2 , class_counts ,  c):\n",
    "    ans = 0\n",
    "    pr_class = class_counts[c]\n",
    "    ans = pr_class \n",
    "    f = open(file,\"r\")\n",
    "    \n",
    "    for line in f:\n",
    "        if 'Lines' in line:\n",
    "                break\n",
    "    for line in f:\n",
    "        for word in line.replace(',','').replace('>','').replace(']','').replace('[','').replace(':','').replace('\\'','').replace('.','').replace('(','').replace(')','').lower().split():\n",
    "        \n",
    "            if (word in word_list1[c]):\n",
    "                \n",
    "                a = len(word_list2[c].keys())\n",
    "                b = len(word_list1[c].keys())\n",
    "                pr_this_feature = (word_list1[c][word] + 1)/(a + b)\n",
    "                ans =  (pr_this_feature)*(ans)*1000\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predict single file probability</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSingle(file, word_list1 ,word_list2):\n",
    "    max_pr = 0\n",
    "    max_class = None\n",
    "    classes = word_list1.keys()\n",
    "    for c in classes:\n",
    "        p = findSingleClassProb(file, word_list1,word_list2 , class_counts, c)\n",
    "        if p > max_pr:\n",
    "            max_pr = p\n",
    "            max_class = c\n",
    "    return max_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> function to choose top words and creates two dictionaries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features(voc , word_list , no_of_features):\n",
    "    \n",
    "    class_no = 0\n",
    "    count_no = 0\n",
    "    word_list1 = {}\n",
    "    word_list2 = {}\n",
    "    \n",
    "    voc = dict((sorted(voc.items(), key = lambda t:t[1], reverse=True))[0:no_of_features])\n",
    "    \n",
    "    \n",
    "            \n",
    "    for c in word_list.keys():\n",
    "        word_list1[c] = {}\n",
    "        word_list2[c] = {}\n",
    "        for top_word in  voc.keys():\n",
    "                \n",
    "\n",
    "                if top_word in word_list[c].keys():\n",
    "                    word_list1[c][top_word] = word_list[c][top_word]\n",
    "                    word_list2[c][top_word] = word_list[c][top_word]\n",
    "\n",
    "                elif top_word not in word_list[c].keys() :\n",
    "                    word_list1[c][top_word] = 0 \n",
    "\n",
    "                \n",
    "    \n",
    "    return word_list1 , word_list2\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> predict function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(voc , word_list , no_of_features):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    word_list1 , word_list2 = top_features(voc , word_list , no_of_features)\n",
    "    \n",
    "    path = \"D:/document_project/20_newsgroups\"\n",
    "    for filename in os.listdir(path):\n",
    "        num = 0\n",
    "        new_path = os.path.join(path, filename)\n",
    "        for textfiles in os.listdir(new_path):\n",
    "\n",
    "            if textfiles not in file_names[filename] and (num < 50):\n",
    "                y_true.append(filename)\n",
    "                file = os.path.join(new_path, textfiles)\n",
    "\n",
    "                y_pred.append( predictSingle(file, word_list1 ,word_list2) )\n",
    "                num +=1   \n",
    "    return y_true , y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> score</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.475"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_true , y_pred = predict(voc , word_list , 6500)\n",
    "accuracy_score(y_true , y_pred)\n",
    "#len(y_true), len(y_pred)\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Manual implementation ends here</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1></h1>\n",
    "<h1> inbuilt implementation starts here</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc1 =  dict((sorted(voc.items(), key = lambda t:t[1], reverse=True))[0:6500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> creating a training dataset in  dataframe</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(columns=voc1.keys())\n",
    "y_train = pd.DataFrame()\n",
    "for filename in os.listdir(path):\n",
    "    new_path = os.path.join(path, filename)\n",
    "    for textfiles in os.listdir(new_path):\n",
    "        file = os.path.join(new_path, textfiles)\n",
    "        f = open(file,\"r\")\n",
    "        single_file_counts = {}\n",
    "        y_train = y_train.append({0 : filename} , ignore_index=True)\n",
    "        for line in f:\n",
    "            if 'Lines' in line:\n",
    "                break\n",
    "        for line in f:\n",
    "            for word in line.replace(',','').replace('>','').replace(']','').replace('[','').replace(':','').replace('\\'','').replace('.','').replace('(','').replace(')','').lower().split():\n",
    "                if word in voc1.keys():\n",
    "                        if word in single_file_counts.keys():\n",
    "                            single_file_counts[word] += 1 \n",
    "                        else :\n",
    "                            single_file_counts[word] = 0\n",
    "        x_train = x_train.append(single_file_counts , ignore_index=True)          \n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> creating a testing dataset in  dataframe</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/document_project/20_newsgroups\"\n",
    "x_test = pd.DataFrame(columns=voc1.keys())\n",
    "y_test = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    num = 0\n",
    "    new_path = os.path.join(path, filename)\n",
    "    for textfiles in os.listdir(new_path):\n",
    "\n",
    "        if textfiles not in file_names[filename] and (num < 50):\n",
    "            y_true.append(filename)\n",
    "            file = os.path.join(new_path, textfiles)\n",
    "            f = open(file,\"r\")\n",
    "            single_file_counts = {}\n",
    "            y_test = y_test.append({ 0 : filename } , ignore_index=True)\n",
    "            for line in f:\n",
    "                if 'Lines' in line:\n",
    "                        break\n",
    "            for line in f:\n",
    "                for word in line.replace(',','').replace('>','').replace(']','').replace('[','').replace(':','').replace('\\'','').replace('.','').replace('(','').replace(')','').lower().split():\n",
    "                         if word in voc1.keys():\n",
    "                            if word in single_file_counts.keys():\n",
    "                                single_file_counts[word] += 1 \n",
    "                            else :\n",
    "                                single_file_counts[word] = 0\n",
    "                                \n",
    "            x_test = x_test.append(single_file_counts , ignore_index=True)\n",
    "            num = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> score</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train , y_train)\n",
    "mnb.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.fillna(0, inplace=True)\n",
    "x_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> </h1>\n",
    "<h1> Inbuilt implemetation ends here</h1>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
